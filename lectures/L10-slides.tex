\input{configuration}
\usepackage{multirow}

\title{Lecture 10 --- Use of Locks, Reentrancy}

\author{Jeff Zarnett \& Patrick Lam \\ \small \texttt{\{jzarnett, patrick.lam\}@uwaterloo.ca}}
\institute{Department of Electrical and Computer Engineering \\
  University of Waterloo}
\date{\today}


\begin{document}

\begin{frame}
  \titlepage

 \end{frame}


\part{Use of Locks}
\frame{\partpage}


\begin{frame}
\frametitle{Use of Locks}

\vspace*{-3em}
In previous courses: learned about locking and how it works.

Before: was enough to avoid bad stuff.

Now: Still important, but not enough.


\end{frame}


\begin{frame}
\frametitle{Critical Section Size}

  \begin{center}
    \input{images/L07-critsec-size-1}
  \end{center}

\end{frame}

\begin{frame}
\frametitle{Critical Section Size}

  \begin{center}
    \input{images/L07-critsec-size-2}
  \end{center}

Critical sections should be as large as they need to be but no larger. 

\end{frame}


\begin{frame}
\frametitle{Rewrite This Block}

  \begin{center}
    \input{images/L07-critsec-size-3}
  \end{center}

Sometimes control flow or other very short statements might get swept into the critical section.

Those should be the exception rather than the rule.

\end{frame}


\begin{frame}[fragile]
\frametitle{Remember the Producer-Consumer Problem?}

Short code example from the producer-consumer problem:
\begin{lstlisting}[language=Rust]
for _j in 0 .. NUM_THREADS {
  // create consumers
  let spaces = spaces.clone();
  let items = items.clone();
  let buffer = buffer.clone();
  threads.push(
    thread::spawn(move || {
      for _k in 0 .. ITEMS_PER_THREAD {
        let permit = block_on(items.acquire());
        let mut buf = buffer.lock().unwrap();
        let current_consume_space = buf.consumer_count;
        let next_consume_space = (current_consume_space+1) % buf.buffer.len();
        let to_consume = *buf.buffer.get(current_consume_space).unwrap();
        buf.consumer_count = next_consume_space;
        spaces.add_permits(1);
        permit.forget();
        consume_item(to_consume);
      }
    })
  );
}

\end{lstlisting}

\end{frame}


\begin{frame}
\frametitle{Previously, More Explicit}

When we used locks in C (or similar), it was easier to identify what's in the critical section, because we had explicit lock and unlock statements. 

The explicit unlock statement, especially, made it much clearer where it ends. 

Now, we don't consider the critical section over until the \texttt{MutexGuard} (returned by \texttt{lock()}) goes out of scope. 

And that happens here at the end of the iteration of the loop.



\end{frame}




\begin{frame}
\frametitle{``Do you think you got him?''}

What I always say is to analyze this closure one statement at a time and look into which of these access shared variables. 

\begin{center}
	\includegraphics[width=\textwidth]{images/gothim.png}
\end{center}

\end{frame}


\begin{frame}
\frametitle{Better to be Safe than Sorry}

In a practical sense, the critical section needs to enclose anything that references \texttt{buf}.

Looking at that code, how did we do?

Rust is good about not letting you access shared data in an uncontrolled way.

But not so good at making sure nothing extra is there.

\end{frame}


\begin{frame}[fragile]
\frametitle{Option 1: Manual Scoping}

\begin{lstlisting}[language=Rust]
for _j in 0 .. NUM_THREADS {
 // create consumers
 let spaces = spaces.clone();
 let items = items.clone();
 let buffer = buffer.clone();
 threads.push(
   thread::spawn(move || {
     for _k in 0 .. ITEMS_PER_THREAD {
       let permit = block_on(items.acquire());
       let to_consume = {
         let mut buf = buffer.lock().unwrap();
         let current_consume_space = buf.consumer_count;
         let next_consume_space = (current_consume_space+1)%buf.buffer.len();
         let to_consume = *buf.buffer.get(current_consume_space).unwrap();
         buf.consumer_count = next_consume_space;
         to_consume
       };
       spaces.add_permits(1);
       permit.forget();
       consume_item(to_consume);
     }
   })
 );
}
\end{lstlisting}

\end{frame}

\begin{frame}
\frametitle{Option 2: Taking Out the Guard}

The other approach to making the \texttt{MutexGuard} go out of scope is to actually call \texttt{drop()} on it. 

\begin{center}
	\includegraphics[width=0.4\textwidth]{images/batty2.jpg}
\end{center}

This is effective in telling the compiler that it is time for this value to die. 

Calling \texttt{drop()} moves ownership of the MutexGuard to the \texttt{drop} function where it will go out of scope and be removed. 

\end{frame}


\begin{frame}
\frametitle{Like for Like}
I applied a similar change the producer code as we just discussed about the consumer. 
 
I added some thread sleeps to the original and modified program so it appears that consuming or producing an item actually takes meaningful work. 

Benchmarks are created with \texttt{hyperfine --warmup 1 -m 5 "cargo run --release"}.
 

\end{frame}


\begin{frame}
\frametitle{Test Results}

Unoptimized version: 2.8 seconds


Optimized version: 1.1 seconds


\end{frame}




\begin{frame}
\frametitle{Locks Have Costs}

Keeping the critical section as small as possible is important because it speeds up performance (reduces the serial portion of your program). 

But that's not the only reason. 

The lock is a resource, and contention for that resource is itself expensive.


\end{frame}


\begin{frame}
\frametitle{Locking Granularity}
Alright, we already know that locks prevent data races.

So we need to use them to prevent data races, but it's not as simple as it sounds. 

We have choices about the granularity of locking, and it is a trade-off.


  \begin{center}
    \includegraphics[scale=0.5]{images/lock-all-the-things}
  \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Coarse-Grained Lock}

  \begin{center}
    \input{images/L07-fine-grained-1}
  \end{center}

\end{frame}

\begin{frame}
\frametitle{Fine-Grained Locks}

  \begin{center}
    \input{images/L07-fine-grained-2}
  \end{center}

\end{frame}

\begin{frame}
\frametitle{Coarse-Grained vs Fine-Grained Locking}

\alert{Coarse-grained} locking is easier to write and harder to mess up, but it can significantly reduce opportunities for parallelism. 

\alert{Fine-grained locking} requires more careful design,
increases locking overhead and is more prone to bugs (deadlock etc).  


Locks' extents constitute their granularity. 

\end{frame}


\begin{frame}
\frametitle{Lock Concerns}

We'll discuss three major concerns when using locks:
  \begin{itemize}
    \item overhead;
    \item contention; and
    \item deadlocks.
  \end{itemize}
  
  
We aren't even talking about under-locking (i.e., remaining race conditions).

\end{frame}

\begin{frame}
\frametitle{Lock Overhead}

  Using a lock isn't free. You pay:
  \begin{itemize}
    \item allocated memory for the locks;
    \item initialization and destruction time; and
    \item acquisition and release time.
  \end{itemize}


  These costs scale with the number of locks that you have.
\end{frame}


\begin{frame}
\frametitle{Lock Contention}

\vspace*{-3em}
 Most locking time is wasted waiting for the lock to become available.\\[1em]
We can fix this by:
\vspace*{-3em}
      \begin{itemize}
        \item making the locking regions smaller (more granular); or
        \item making more locks for independent sections.
      \end{itemize}


\end{frame}

\begin{frame}
\frametitle{Deadlocks}

Finally, the more locks you have, the more you have to worry about deadlocks.

\begin{center}
	\includegraphics[width=0.5\textwidth]{images/deadlock.jpg}
\end{center}

As you know, the key condition for a deadlock is waiting for a lock held by process $X$ while holding a lock held by process $X'$. ($X = X'$ is allowed).

\end{frame}


\begin{frame}
\frametitle{Deadlocks}



Okay, in a formal sense, the four conditions for deadlock are:

\begin{enumerate}
	\item \textbf{Mutual Exclusion}
	\item \textbf{Hold-and-Wait}
	\item \textbf{No Preemption}
	\item \textbf{Circular-Wait}
\end{enumerate}

\end{frame}


\begin{frame}[fragile]
\frametitle{Simple Deadlock Example}

Consider, for instance, two processors trying to get two locks.

\begin{center}
  \begin{tabular}{ll}
\begin{minipage}{.4\textwidth}
      {\bf Thread 1}

      \verb+Get Lock 1+

      \verb+Get Lock 2+

      \verb+Release Lock 2+

      \verb+Release Lock 1+
\end{minipage} & 
\begin{minipage}{.4\textwidth}
      {\bf Thread 2}

      \verb+Get Lock 2+

      \verb+Get Lock 1+

      \verb+Release Lock 1+

      \verb+Release Lock 2+
\end{minipage}
\end{tabular}
\end{center}


\end{frame}

\begin{frame}[fragile]
\frametitle{Avoiding Deadlocks}

To avoid deadlocks, always be careful if your code {\bf acquires a lock while holding one}.  
  
You have two choices: (1) ensure consistent ordering in acquiring locks; or (2) use trylock.

As an example of consistent ordering:
\begin{center}
\begin{tabular}{ll}
\begin{minipage}{.4\textwidth}
  \begin{lstlisting}[language=Rust]
let mut thing1 = l1.lock().unwrap()
let mut thing2 = l2.lock().unwrap()
// protected code
// locks dropped when going out of scope
\end{lstlisting}
\end{minipage}&
\begin{minipage}{.4\textwidth}
\begin{lstlisting}[language=Rust]
let mut thing1 = l1.lock().unwrap()
let mut thing2 = l2.lock().unwrap()
// protected code
// locks dropped when going out of scope
  \end{lstlisting}
\end{minipage}
\end{tabular}
\end{center}

\end{frame}



\begin{frame}[fragile]
\frametitle{Trylock}

Alternately, you can use trylock. 

Recall that Pthreads' {\tt trylock} returns 0 if it gets the lock. 

But if it doesn't, your thread doesn't get blocked. 

  \begin{lstlisting}[language=Rust]
    loop {
        let mut m1 = l1.lock().unwrap();
        let m2 = l2.try_lock();
        if m2.is_ok() {
            *m1 += amount;
            *m2.unwrap() -= amount;
            break;
        } else {
            println!("try_lock failed");
            // Go around the loop again and try again
        }
    }
  \end{lstlisting}
  
This prevents the hold and wait condition.

\end{frame}



\begin{frame}
\frametitle{There Can Be Only One}

\begin{center}
	\includegraphics[width=0.8\textwidth]{images/giantlock.jpg}\\
	\hfill Photo Credit: Craig Middleton\footnote{\url{https://www.flickr.com/photos/craigmiddleton/3579668458}}
\end{center}


\end{frame}


\begin{frame}
\frametitle{Coarse-Grained Locking}

One way of avoiding problems due to locking is to use few locks
(1?). 

This is \emph{coarse-grained locking}; it does have a couple of advantages:
  \begin{itemize}
    \item it is easier to implement;
    \item with one lock, there is no chance of deadlocking; and
    \item it has the lowest memory usage and setup time possible.
  \end{itemize}

Your parallel program will quickly become sequential.


\end{frame}

\begin{frame}
\frametitle{Python}

Python puts a lock around the whole interpreter (known as the
\emph{global interpreter lock}).  

This is the main reason (most) scripting languages have poor parallel performance; Python's just an example.

Two major implications:
\begin{itemize}
\item The only performance benefit you'll see from threading is if one of the threads is
      waiting for I/O.
\item But: any non-I/O-bound threaded program will be {\bf slower} than the sequential
      version (plus, the interpreter will slow down your system).
\end{itemize}


\end{frame}

\begin{frame}
\frametitle{Fine-Grained Locking}

On the other end of the spectrum is \emph{fine-grained locking}. 


\begin{center}
	\includegraphics[width=0.4\textwidth]{images/tinylocks.jpeg}
\end{center}


\end{frame}

\begin{frame}
\frametitle{Fine-Grained Locking} 

The big advantage: it maximizes parallelization in your program.

However, it also comes with a number of disadvantages:
  \begin{itemize}
    \item if your program isn't very parallel, it'll be mostly wasted memory and setup time;
    \item plus, you're now prone to deadlocks; and
    \item fine-grained locking is generally more error-prone (be sure you grab the right lock!)
  \end{itemize}

\end{frame}

\begin{frame}
\frametitle{Lock Scope Examples}


    Databases may lock fields / records / tables. (fine-grained $\rightarrow$ coarse-grained).

    You can also lock individual objects (but beware: sometimes you need transactional guarantees.)



\end{frame}

\part{Reentrancy}
\frame{\partpage}


\begin{frame}
\frametitle{Out and In Again}

\begin{center}
	\includegraphics[width=0.5\textwidth]{images/revolvingdoor.jpg}
\end{center}

\end{frame}


\begin{frame}[fragile]
\frametitle{Non-Reentrant C Function}

The trivial example of a non-reentrant C function:

\begin{lstlisting}[language=C]
int tmp;

void swap( int x, int y ) {
    tmp = y;
    y = x;
    x = tmp;
}
\end{lstlisting}

Why is this non-reentrant?

How can we make it reentrant?

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Reentrancy}

  
    $\Rightarrow$ A function can be suspended in the middle and \\ {\bf re-entered}
      (called again) before the previous execution returns.\\[1em]
    
     Does not always mean {\bf thread-safe} (although it usually is).
      \begin{itemize}
        \item Recall: {\bf thread-safe} is essentially ``no data races''.
      \end{itemize}
 ~\\[1em]
  Moot point if the function only modifies local data, e.g. {\tt sin()}.
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Reentrancy Example}

  

  Courtesy of Wikipedia (with modifications):
  \begin{lstlisting}[language=C]
int t;
 
void swap(int *x, int *y) {
    t = *x;
    *x = *y;
    // hardware interrupt might invoke isr() here!
    *y = t;
}
 
void isr() {
    int x = 1, y = 2;
    swap(&x, &y);
}
...
int a = 3, b = 4;
...
    swap(&a, &b);
  \end{lstlisting}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Reentrancy Example---Explained (a trace)}

  
  \begin{lstlisting}[language=C]
call swap(&a, &b);
 t = *x;                // t = 3 (a)
 *x = *y;               // a = 4 (b)
 call isr();
    x = 1; y = 2;
    call swap(&x, &y)
    t = *x;             // t = 1 (x)
    *x = *y;            // x = 2 (y)
    *y = t;             // y = 1
 *y = t;                // b = 1

Final values:
a = 4, b = 1

Expected values:
a = 4, b = 3
  \end{lstlisting}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Reentrancy Example, Fixed}

  
  \begin{lstlisting}[language=C][escapechar=@]
int t;
 
void swap(int *x, int *y) {
    int s;
 
    @\alert{s = t}@;  // save global variable
    t = *x;
    *x = *y;
    // hardware interrupt might invoke isr() here!
    *y = t;
    @\alert{t = s}@;  // restore global variable
}
 
void isr() {
    int x = 1, y = 2;
    swap(&x, &y);
}
...
int a = 3, b = 4;
...
    swap(&a, &b);
  \end{lstlisting}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Reentrancy Example, Fixed---Explained (a trace)}

  
  \begin{lstlisting}[language=C]
call swap(&a, &b);
s = t;                 // s = UNDEFINED
t = *x;                // t = 3 (a)
*x = *y;               // a = 4 (b)
call isr();
    x = 1; y = 2;
    call swap(&x, &y)
    s = t;             // s = 3
    t = *x;            // t = 1 (x)
    *x = *y;           // x = 2 (y)
    *y = t;            // y = 1
    t = s;             // t = 3
*y = t;                // b = 3
t = s;                 // t = UNDEFINED

Final values:
a = 4, b = 3

Expected values:
a = 4, b = 3
  \end{lstlisting}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Interrupt Handling}


Remember that in things like interrupt subroutines (ISRs) having the code be reentrant is very important. 

Interrupts can get interrupted by higher priority interrupts and when that happens the ISR may simply be restarted, or we pause and resume.

Either way, if the code is not reentrant we will run into problems.


\end{frame}


\begin{frame}
\frametitle{Functional Programming}

Interestingly, functional programming languages (NOT procedural like C) such as Scala and so on, lend themselves very nicely to being parallelized. 

Why? 

Because a purely functional program has no side effects and they are very easy to parallelize.

Any impure function has to indicate that in its function signature.

\end{frame}



\begin{frame}
\frametitle{Functional Programming}

\begin{center}
	\includegraphics[width=0.8\textwidth]{images/functional.png}
\end{center}
{\small \url{https://www.fpcomplete.com/blog/2017/04/pure-functional-programming}}


\end{frame}



\begin{frame}
\frametitle{Joel on Functional}

\begin{quote}
\textit{Without understanding functional programming, you can't invent MapReduce, the algorithm that makes Google so massively scalable. The terms Map and Reduce come from Lisp and functional programming. MapReduce is, in retrospect, obvious to anyone who remembers from their 6.001-equivalent programming class that purely functional programs have no side effects and are thus trivially parallelizable.}
\end{quote}
\hfill --- Joel Spolsky


\end{frame}

\begin{frame}
\frametitle{OOP: \texttt{getBrain().damage()}}

Object oriented programming kind of gives us some bad habits in this regard. 

We tend to make a lot of \texttt{void} methods. 

In functional programming these don't really make sense, because if it's purely functional, then there are some inputs and some outputs. 

If a function returns nothing, what does it do? 

For the most part it can only have side effects which we would generally prefer to avoid if we can, if the goal is to parallelize things. 

\end{frame}

\begin{frame}
\frametitle{Functional Programming in Your C++}

{\tt algorithms} has been part of C++ since C++11.\\

C++17 (not well supported yet) introduces parallel and vectorized {\tt algorithms};\\
\qquad you specify an execution policy, compiler does it:\\
\qquad ({\tt sequenced}, {\tt parallel},
or {\tt parallel\_unsequenced}).\\[1em]

Some examples of algorithms:\\
\qquad {\tt sort}, {\tt reverse}, {\tt is\_heap}\ldots\\[1em]

Other examples of algorithms:\\
\qquad {\tt for\_each\_n}, {\tt exclusive\_scan}, {\tt reduce}\\[1em]
If you know functional programming (e.g. Haskell), these are:\\
\qquad {\tt map}, {\tt scanl}, {\tt fold1/foldl1}.

\end{frame}

\begin{frame}
\frametitle{Side Effects}

Side effects are sort of undesirable (and definitely not functional), but not necessarily bad. 

Printing to console is unavoidably making use of a side effect, but it's what we want. 

We don't want to call print reentrantly, because two threads trying to write at the same time to the console would result in jumbled output. 

Or alternatively, restarting the print routine might result in some doubled characters on the screen.

\end{frame}

\begin{frame}
\frametitle{Purity}

The notion of purity is related to side effects.

A function is \emph{pure} if it has no side effects and if its outputs depend solely on its inputs.

Pure functions should be implemented as thread-safe and reentrant.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Previous Example: thread-safety}

  
  Is the previous reentrant code also thread-safe?\\

  (This is more what we're concerned about in this course.)\\[1em]

  Let's see:
  \begin{lstlisting}[language=C]
int t;

void swap(int *x, int *y) {
    int s;
 
    s = t;  // save global variable
    t = *x;
    *x = *y;
    // hardware interrupt might invoke isr() here!
    *y = t;
    t = s;  // restore global variable
}
  \end{lstlisting}
  
  Consider two calls: {\tt swap(a, b)}, {\tt swap(c, d)} with\\
  \quad {\tt a = 1, b = 2, c = 3, d = 4}.
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Previous Example: thread-safety trace}

  
  \begin{lstlisting}[language=C]
global: t

/* thread 1 */                       /* thread 2 */
a = 1, b = 2;
s = t;    // s = UNDEFINED
t = a;    // t = 1
                                     c = 3, d = 4;
                                     s = t;    // s = 1
                                     t = c;    // t = 3
                                     c = d;    // c = 4
                                     d = t;    // d = 3
a = b;    // a = 2
b = t;    // b = 3
t = s;    // t = UNDEFINED
                                     t = s;    // t = 1

Final values:
a = 2, b = 3, c = 4, d = 3, t = 1

Expected values:
a = 2, b = 1, c = 4, d = 3, t = UNDEFINED
  \end{lstlisting}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Another Definition of Thread-Safe Functions}

  

\begin{quote}
  ``A function whose effect, when called by two or more threads, is guaranteed to
  be as if the threads each executed the function one after another, in an
  undefined order, even if the actual execution is interleaved.''
\end{quote}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}
\frametitle{Functional Rust?}

Rust does not force a functional style upon your program.

It discourages mutability of data, which is sort of functional-ish.

Internal mutability is a thing, but discouraged.

\end{frame}




\end{document}

