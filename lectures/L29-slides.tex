\input{configuration}
\usepackage{soul}

\title{Lecture 28 --- Liar, Liar }

\author{Patrick Lam \& Jeff Zarnett \\ \small \texttt{patrick.lam@uwaterloo.ca jzarnett@uwaterloo.ca}}
\institute{Department of Electrical and Computer Engineering \\
  University of Waterloo}
\date{\today}


\begin{document}

\begin{frame}
  \titlepage
 \end{frame}


\part{Liar, Liar}

\begin{frame}
\frametitle{Liar, Liar}

\begin{center}
	\includegraphics[width=0.75\textwidth]{images/Liar-Liar.jpg}
\end{center}


\end{frame}

\begin{frame}
\frametitle{Sampling Based Profiling}

Let's open with a video that illustrates one of the problems with sampling-based profiling:

\begin{center}
	\url{https://www.youtube.com/watch?v=jQDjJRYmeWg}
\end{center}

Is this fake?

\end{frame}


\begin{frame}
\frametitle{Assumptions}

The main assumptions underlying sampling are that:

Samples are ``random''; and 

The sample distribution approximates the actual time-spent distribution.

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{In the Criminal Justice System...}
  
  
    Who can we trust?\\[1em]
    
    \begin{center}
	\includegraphics[width=0.6\textwidth]{images/lawandorder.jpg}
	\end{center}
\end{frame}


\part{Lies from Metrics}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \partpage
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Lying perf counters}
  
  
    While app-specific metrics can lie too,\\
    mostly we'll talk about CPU perf counters.
  



  \begin{center}
    Reference: Paul Khuong,\\
  \tiny
  \url{http://www.pvk.ca/Blog/2014/10/19/performance-optimisation-~-writing-an-essay/}

  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{mfence}


    We've talked about {\tt mfence}.\\
    Used in spinlocks, for instance.\\[2em]
    Profiles said: spinlocking didn't take much time.\\
    Empirically: eliminating spinlocks = better than expected!
    
  
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Exploring the lie}

  
    Next step: create microbenchmarks.\\[1em]
    Memory accesses to uncached locations,\\
    or computations,\\[1em]
    surrounded by store pairs/mfence/locks.\\[1em]
    Use perf to evaluate impact of mfence vs lock.
  

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{perf for lock}

    \begin{lstlisting}
$ perf annotate -s cache_misses
[...]
    0.06 :        4006b0:       and    %rdx,%r10
    0.00 :        4006b3:       add    $0x1,%r9
    ;; random (out of last level cache) read
    0.00 :        4006b7:       mov    (%rsi,%r10,8),%rbp
   30.37 :        4006bb:       mov    %rcx,%r10
    ;; foo is cached, to simulate our internal lock
    0.12 :        4006be:       mov    %r9,0x200fbb(%rip)
    0.00 :        4006c5:       shl    $0x17,%r10
    [... Skipping arithmetic with < 1% weight in the profile]
    ;; locked increment of an in-cache "lock" byte
    1.00 :        4006e7:       lock incb 0x200d92(%rip)
   21.57 :        4006ee:       add    $0x1,%rax
    [...]
    ;; random out of cache read
    0.00 :        400704:       xor    (%rsi,%r10,8),%rbp
   21.99 :        400708:       xor    %r9,%r8
    [...]
    ;; locked in-cache decrement
    0.00 :        400729:       lock decb 0x200d50(%rip)
   18.61 :        400730:       add    $0x1,%rax
    [...]
    0.92 :        400755:       jne    4006b0 <cache_misses+0x30>
    \end{lstlisting}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{lock's effects}

  
    Reads take 30 + 22 = 52\% of runtime\\
    Locks take 19 + 21 = 40\%.
  

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{perf for mfence}

  \begin{lstlisting}
$ perf annotate -s cache_misses
[...]
    0.00 :        4006b0:       and    %rdx,%r10
    0.00 :        4006b3:       add    $0x1,%r9
    ;; random read
    0.00 :        4006b7:       mov    (%rsi,%r10,8),%rbp
   42.04 :        4006bb:       mov    %rcx,%r10
    ;; store to cached memory (lock word)
    0.00 :        4006be:       mov    %r9,0x200fbb(%rip)
    [...]
    0.20 :        4006e7:       mfence 
    5.26 :        4006ea:       add    $0x1,%rax
    [...]
    ;; random read
    0.19 :        400700:       xor    (%rsi,%r10,8),%rbp
   43.13 :        400704:       xor    %r9,%r8
    [...]
    0.00 :        400725:       mfence 
    4.96 :        400728:       add    $0x1,%rax
    0.92 :        40072c:       add    $0x1,%rax
    [...]
    0.36 :        40074d:       jne    4006b0 <cache_misses+0x30>
  \end{lstlisting}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{mfence's effects}

  
    Looks like the reads take 85\% of runtime,\\
    while the mfence takes 15\% of runtime.
  

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Bigger picture}

  
    Must also look at total \# of cycles.\\[1em]
    \begin{tabular}{lr}
    No atomic/fence:& 2.81e9 cycles\\
    lock inc/dec: & 3.66e9 cycles\\
    mfence: & 19.60e9 cycles
    \end{tabular}
~\\[1em]
    That 15\% number is a total lie.
  
  \begin{center}
	\includegraphics[width=0.3\textwidth]{images/maury.jpg}
\end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Conclusions}

  
    \begin{itemize}
    \item mfence underestimated;
      \item lock overestimated.
    \end{itemize}
    ~\\
    Why? \\[1em]
    mfence = pipeline flush,\\
    costs attributed to instructions being flushed.\\
  

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\part{The Long Tail}

\begin{frame}
\partpage
\end{frame}



\begin{frame}
\frametitle{The Long Tail}

Suppose we have a task that's going to get distributed over multiple computers (like a search). 

If we look at the latency distribution, the problem is mostly that we see a long tail of events. 

When we are doing a computation or search where we need all the results, we can only go as the slowest step.

\end{frame}



\begin{frame}
\frametitle{Grab the Tiger by the Tail}

\begin{center}
	\includegraphics[width=0.85\textwidth]{images/disk_tail.png}
\end{center}

\end{frame}



\begin{frame}
\frametitle{Explaining the Peaks}

\begin{enumerate}
	\item Found in RAM
	\item Disk Cache
	\item Disk
	\item and above... very strange!
\end{enumerate}

\end{frame}



\begin{frame}
\frametitle{Why 250, 500, 750, 1000?}

Answer: CPU throttling!

This was happening on 25\% of disk servers at Google, for an average of half an hour a day!

\end{frame}



\begin{frame}
\frametitle{Faster than a Speeding Bullet}

Another problem with sampling,
this time from Lucene:

\begin{center}
	\includegraphics[width=0.7\textwidth]{images/perf-sample.png}
\end{center}

Why is perf limited to 100~KHz?

\end{frame}



\begin{frame}
\frametitle{Processing Interrupts}

Answer: perf samples are done with interrupts (slow). 

If you crank up the rate of interrupts, before long, you are spending all your time handling the interrupts rather than doing useful work.

SHIM gets around this by being more invasive.

This produces a bunch of data which can be dealt with later.

\end{frame}

\part{Lies from Counters}

\begin{frame}
\partpage
\end{frame}


\begin{frame}
\frametitle{Counter This}

Rust compiler hackers were trying to include
support for hardware performance counters (what perf reports).

To make counters as deterministic as possible:
\begin{itemize}
\item disable Address Space Layout Randomization (randomized pointer addresses affect hash layouts);
\item subtract time spent processing interrupts (IRQs);
\item profile one thread only (if you can, in your context).
\end{itemize}


\end{frame}


\begin{frame}
\frametitle{Problem?}

\begin{center}
\includegraphics{images/L27-eb-tweet.png}
\end{center}


\end{frame}



\part{Lies about Calling Context}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \partpage
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}
\frametitle{Profilers, Lying?}

This part is somewhat outdated now, as it's a pretty specific technical problem that especially arises under the {\tt gprof} tool. 

It's still a good example of lying tools, though.

  \begin{center}
    Reference: Yossi Kreinin,\\
  \tiny
  \url{http://www.yosefk.com/blog/how-profilers-lie-the-cases-of-gprof-and-kcachegrind.html}
  \end{center}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{gprof said what?}

gprof uses two C standard-library functions: {\bf profil()} and {\bf mcount()}.

\begin{itemize}
\item {\bf profil()}: asks glibc to record which instruction 
  is currently executing (100$\times$/second).
\item {\bf mcount()}: records call graph edges; called by {\tt -pg} instrumentation.
\end{itemize}


  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Getting it Wrong}

Hence, {\bf profil} information is statistical, while {\bf mcount}
information is exact.  

gprof can draw unreliable inferences. 

If you have a method \texttt{easy} and a method \texttt{hard}, each of which is called once, and \texttt{hard} takes up almost all the CPU time...


gprof might divide total time by 2 and report bogus results.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{What's Wrong?}

The following results from gprof are suspect (among others):

\begin{itemize}
  \item contribution of children to parents;
  \item total runtime spent in self+children;
\end{itemize}


When are call graph edges right?
Two cases:

    \begin{itemize}
    \item functions with only one caller
      (e.g. {\tt f()} only called by {\tt g()}); or,
    \item functions which always take the same time to complete
      (e.g. {\tt rand()}).
    \end{itemize}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}
\frametitle{Other Tools}

Next, we'll talk about callgrind/KCacheGrind.

KCacheGrind is a frontend to callgrind. callgrind gives better information, but imposes more overhead.


KCacheGrind works properly on the earlier hard/easy example, but we can still deceive it with more complicated examples.

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Lies  summary}
  
    Some results are exact;\\
    some results are sampled;\\
    some results are interpolated.\\[1em]

    If you understand the tool, \\
    you understand where it can go wrong.\\[1em]

    Understand your tools!
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


