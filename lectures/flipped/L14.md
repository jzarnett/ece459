# Lecture 14 — Memory Consistency

## Compiler reordering

Question: if accessing `thing.y` takes a long time, how can we speed up the
following code?

```rust
let x = thing.y;
println!("x = {}", x);
z = z + 1;
a = b + c;
```

## Hardware reordering

Question: what are possible final values for `y`?

```rust
initial state: x = 0, y = 1

THREAD 1        THREAD2
y = 3;          if x == 1 {
x = 1;              y *= 2;
                }
```

Answer can be found in the [Rustonomicon
book](https://doc.rust-lang.org/nomicon/atomics.html#hardware-reordering)

To understand that, a good analogy might be: a multicore system is a bit like a
group of programmers collaborating on a project using a bizarre kind of source
control strategy (See
[here](https://preshing.com/20120710/memory-barriers-are-like-source-control-operations/),
which also talks about memory barriers that we'll mention below)

## Memory consistency models

Talk about *memory barrier* or *fence*.

* Sequentially Consistent (SeqCst)

> "... the result of any execution is the same as if the operations of all the
> processors were executed in some sequential order, and the operations of each
> individual processor appear in this sequence in the order specified by its
> program." — Leslie Lamport

* Acquire / Release

From the [Rustonomicon's page](https://doc.rust-lang.org/nomicon/atomics.html#acquire-release)

> When thread A releases a location in memory and then thread B subsequently
> acquires the same location in memory, causality is established. Every write
> (including non-atomic and relaxed atomic writes) that happened before A's
> release will be observed by B after its acquisition.

* Relaxed

Rarely used, weakest, can be freely re-ordered, but still atomic. Can you think
of a use case? (One answer can be found in the lecture notes)

Exercise: check the runtime between `SeqCst` and `Relaxed`

```rust
/*
[dependencies]

// rustexplorer only recognize dev
// so copy release profile and make it as dev
[profile.dev]
opt-level = 3
debug = false
split-debuginfo = '...'  # Platform-specific.
debug-assertions = false
overflow-checks = false
lto = false
panic = 'unwind'
incremental = false
codegen-units = 16
rpath = false
*/

use std::{
    sync::atomic::{AtomicI64, Ordering},
    thread, time::Instant,
};

static CNT: AtomicI64 = AtomicI64::new(0);
static LARGE_CNT: usize = 10_000_000;

fn main() {
    let mut handles = vec![];
    CNT.store(0, Ordering::SeqCst);
    let start = Instant::now();
    for _ in 0..4 {
        handles.push(thread::spawn(|| {
            for _ in 0..LARGE_CNT {
                CNT.fetch_add(1, Ordering::SeqCst);
                // CNT.fetch_add(1, Ordering::Relaxed);
                // ^ Change the line and do you see any runtime difference?
                // How about on your own laptop?
            }
        }));
    }
    for h in handles {
        h.join().unwrap();
    }
    println!("{:?}", start.elapsed());
}
```

## Other

Question: memory consistency model vs. cache coherence?

*May be think of it before you read the answer below*

Answer might be found in <https://ieeexplore.ieee.org/document/546611>

By searching for "**CACHE COHERENCE PROTOCOLS**", it will tell you that there
are several definitions of cache coherence, but not all of them guarantee strong
consistency.

For the difference, as the paper says, "We view a cache coherence protocol as
simply a mechanism to propagate a newly written value. The memory consistency
model is the policy that places the bounds on when the value can be propagated
to a given processor."
