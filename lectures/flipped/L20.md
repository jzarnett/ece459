# Lecture 20 â€” Self-Optimizing Software

## Roadmap

This lecture is about what software can do on-the-fly to make itself run faster.

## Initial exercise

How can software make *itself* faster?

The quick summary is:
* caching: easy mode
* observe and change: modify configuration/data layout on the fly
* genetic algorithms: one way of improving configurations/data layouts
* just-in-time compilation/optimization
 + lock optimization
 + on-stack replacement
* binary rewriting

## Observe and change: experimentation [live coding 5 minutes, experimentation 10 minutes]

Exercise: can we think of real-life software examples where observe-and-change
makes sense? (Talk about the ones in the lecture notes: data structure
selection; database query processing; indexes; external services).

Consider the [im docs](https://docs.rs/im/14.3.0/im/):

> For instance, Vec beats everything at memory usage, indexing and operations
> that happen at the back of the list, but is terrible at insertion and removal,
> and gets worse the closer to the front of the list you get. VecDeque adds a
> little bit of complexity in order to make operations at the front as efficient
> as operations at the back, but is still bad at insertion and especially
> concatenation. Vector adds another bit of complexity, and could never match
> Vec at what it's best at, but in return every operation you can throw at it
> can be completed in a reasonable amount of time - even normally expensive
> operations like copying and especially concatenation are reasonably cheap when
> using a Vector.

> It should be noted, however, that because of its simplicity, Vec actually
> beats Vector even at its strongest operations at small sizes, just because
> modern CPUs are hyperoptimised for things like copying small chunks of
> contiguous memory - you actually need to go past a certain size (usually in
> the vicinity of several hundred elements) before you get to the point where
> Vec isn't always going to be the fastest choice. Vector attempts to overcome
> this by actually just being an array at very small sizes, and being able to
> switch efficiently to the full data structure when it grows large enough.
> Thus, Vector will actually be equivalent to Vec until it grows past the size
> of a single chunk.

Exercise: Look at `lectures/live-coding/L20a-skel`, experiment with different
data structures and operation mixes, and try to understand what is fast when.

Exercise: Look at `lectures/live-coding/L20c-skel`, choose a good initial size.

## Lock optimization simulation [15 minutes]

Exercise: Look at `flipped/L20/lockopt-skel`, see how you can optimize the locks
given the workload.

## Rewriting the binary [30 minutes]

note: L20c-skel basically does this already.

Have the program modify itself to add inline annotations, recompile itself and
re-invoke the new version of itself. You may find the [build script
examples](https://doc.rust-lang.org/cargo/reference/build-script-examples.html)
useful.

# After-action report, plam, 13 Mar 2023

Did all the things, pretty much, and did not get to L21 (which was the plan).
To fix for next time: L20c-skel needs longer runs; to be able to handle changes in whitespace
in the target line because VS Code changes whitespace; and should not
put extra blank lines.

We also experimented with lockopt-skel but changed the locks to atomics and
saw how that worked. We should be consistent with using the flipped/ and live-coding
directories for things here.

We did not talk about *why* VecDeque worked better.
