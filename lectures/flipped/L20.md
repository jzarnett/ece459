# Lecture 20 â€” Self-Optimizing Software

## Roadmap

This lecture is about what software can do on-the-fly to make itself run faster.

## Initial exercise

How can software make *itself* faster?

The quick summary is:
* caching: easy mode
* observe and change: modify configuration/data layout on the fly
* genetic algorithms: one way of improving configurations/data layouts
* just-in-time compilation/optimization
  * lock optimization
  * on-stack replacement
* binary rewriting

## Observe and change: experimentation [live coding 5 minutes, experimentation 10 minutes]

Exercise: can we think of real-life software examples where observe-and-change
makes sense? (Talk about the ones in the lecture notes: data structure
selection; database query processing; indexes; external services).

Consider the [im docs](https://docs.rs/im/15.1.0/im/index.html):

> For instance, Vec beats everything at memory usage, indexing and operations
> that happen at the back of the list, but is terrible at insertion and removal,
> and gets worse the closer to the front of the list you get. VecDeque adds a
> little bit of complexity in order to make operations at the front as efficient
> as operations at the back, but is still bad at insertion and especially
> concatenation. Vector adds another bit of complexity, and could never match
> Vec at what it's best at, but in return every operation you can throw at it
> can be completed in a reasonable amount of time - even normally expensive
> operations like copying and especially concatenation are reasonably cheap when
> using a Vector.

> It should be noted, however, that because of its simplicity, Vec actually
> beats Vector even at its strongest operations at small sizes, just because
> modern CPUs are hyperoptimised for things like copying small chunks of
> contiguous memory - you actually need to go past a certain size (usually in
> the vicinity of several hundred elements) before you get to the point where
> Vec isn't always going to be the fastest choice. Vector attempts to overcome
> this by actually just being an array at very small sizes, and being able to
> switch efficiently to the full data structure when it grows large enough.
> Thus, Vector will actually be equivalent to Vec until it grows past the size
> of a single chunk.

Exercise: Look at `lectures/live-coding/L20a-skel`, experiment with different
data structures and operations, and try to understand what is fast and when.

Exercise: Look at `lectures/live-coding/L20c-skel`, choose a good initial size.

## Lock optimization simulation [15 minutes]

Exercise: Look at `flipped/L20/lockopt-skel`, see how you can optimize the locks
given the workload.

## Rewriting the binary [30 minutes]

A real example is in <https://github.com/aengelke/binopt>. Based on `func`, it
generates a new version `new_func` which can be later optimized. Although the
call to `new_func` is coded as `new_func(8, 16)`, the second parameter `16`,
however, will be changed to `42` when it is executed. Note that this change
happens at runtime using `binopt_cfg_set_parami`. (Question: why not `if-else`?)

Rewriting the binary is difficult, we will try rewriting the source code
instead. The steps are similar though.

Have the program (also in `L20c-skel`) modify itself to add inline annotations,
recompile itself and re-invoke the new version of itself. You may find the
[build script
examples](https://doc.rust-lang.org/cargo/reference/build-script-examples.html)
useful, especially how it uses `Command::new`.

# After-action report, plam, 13 Mar 2023

Did all the things, pretty much, and did not get to L21 (which was the plan). To
fix for next time: L20c-skel needs longer runs; to be able to handle changes in
whitespace in the target line because VS Code changes whitespace; and should not
put extra blank lines.

We also experimented with lockopt-skel but changed the locks to atomics and saw
how that worked. We should be consistent with using the flipped/ and live-coding
directories for things here.

We did not talk about *why* VecDeque worked better.

# After-action report, huanyi, 26Feb24

Discussed each strategy shortly. Did all exercises except the rewriting binary
one. With the fixed L20a code, now the elapsed times make much more sense. It
turns out `Vec` is extremely efficient when inserting at the end, but it is
terrible when removing from the front. `VecDeque` performs generally well for
every workload. I did not try `Vector`, which seems to be not maintained
anymore.

# After-action report, huanyi, 01Mar24

Finished the rest
